{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was done as one of the requirements to complete the nanodegree data analyst course. This project is titled Wrangle and Analyse data. The dataset provided for is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. In this project, i gathered, assessed and cleaned the dataset provided. kindly find below the project overview\n",
    "\n",
    "Project Steps Overview\n",
    "\n",
    "Step 1: Gathering data\n",
    "\n",
    "Step 2: Assessing data\n",
    "\n",
    "Step 3: Cleaning data\n",
    "\n",
    "Step 4: Storing data\n",
    "\n",
    "Step 5: Analyzing, and visualizing data\n",
    "\n",
    "Step 6: Reporting\n",
    "\n",
    "your data wrangling efforts\n",
    "your data analyses and visualizations\n",
    "\n",
    "-Gathering Data\n",
    "Three pieces of data needed to be the gathered through various means. The first piece of data needed is the twitter_archive_enhanced.csv which has been provided by udacity and i downloaded it manually.\n",
    "\n",
    "Second piece of data is (image_predictions.tsv).It is hosted on Udacity's servers and i downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "Third piece of data is gotten directly from the twitter API. I needed to query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. I couldn't query the twitter API because after numerous request twitter never granted me access. There i made do with the file provided by Udacity.\n",
    "\n",
    "-Assessing data\n",
    "After gathering all these three pieces of data, i assessed them visually and programmatically for quality and tidiness issues. I could identify and  document at eight (8) quality issues and two (4) tidiness issues. This issues are listed below:\n",
    "\n",
    "Quality issues:\n",
    "\n",
    ".Issues with the timestamp, retweeted_status_timestamp, source column on the tweet_arc dataframe.\n",
    "\n",
    ".Column names on the tweet_arc dataframe should be more explicit and clean.\n",
    "\n",
    ".There is an html link after the exact text in the text column of the tweet_arc dataframe.\n",
    "\n",
    ".Preceeding html link on the source column values(remove the html link) on the tweet_arc dataframe.\n",
    "\n",
    ".Dogs should be rated over 10 in the rating_denominator column. We have invalid values like(0,2, 11, 50, 80)\n",
    "\n",
    ".Float values in the rating_denominator column.\n",
    "\n",
    ".Couple of Values in the rating_numerator seems outrageous on the tweet_arc dataframe\n",
    "\n",
    ".Wrong datatype on the created_at column on the tweet_json dataframe\n",
    "\n",
    "Tidiness issues:\n",
    "\n",
    ".Merge all the columns of the four dog stages to a single column and drop the 4 other columns in the table.\n",
    "\n",
    ".Dissolve the predictions and confidence level columns in the prediction and confidence level column.\n",
    "\n",
    ".Drop columns no longer needed for further analysis.\n",
    "\n",
    ".Merge all 3 dataframes into one.\n",
    "\n",
    "-Cleaning data\n",
    "\n",
    "In this session i cleaned all of the issues i raised and  documented while assessing. Before cleaning, a copy of the original data was made. Also during cleaning the define-code-test framework was used, this enables orderliness and maintained uniformity on the notebook.\n",
    "\n",
    "-Storing data\n",
    "\n",
    "Once data has been cleaned next is to store the cleaned master DataFrame in a CSV file with the main one named twitter_archive_master.csv\n",
    "\n",
    "-Analyzing, and visualizing data\n",
    "\n",
    "I produced at three (3) insights and a/some visualization for each. I also documented the piece of assessed and cleaned data used to make each analysis and visualization. the insights include:\n",
    "   1. What is the most popular tweet source or most used device?\n",
    "\n",
    "   2. Which of the dog stages appeared most and what it could mean.\n",
    "\n",
    "   3. What month has the highest amount of retweets?\n",
    "   \n",
    " In conclusion, i feel gratefull for being granted this opportunity to learn on Udacity. Its all new to me , but i keep gaining more knowlegde by the day, these projects has helped tremendously. This is the begining to a fun and cleaning and wrangling filled career."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
